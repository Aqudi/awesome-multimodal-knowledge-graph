**Resource List w/ Abstract**

- [Paper](#paper)
    - [Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries [pdf]](#building-a-large-scale-multimodal-knowledge-base-system-for-answering-visual-queries-pdf)
    - [Towards Building Large Scale Multimodal Domain-Aware Conversation Systems [pdf]](#towards-building-large-scale-multimodal-domain-aware-conversation-systems-pdf)
    - [A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning [pdf]](#a-multimodal-translation-based-approach-for-knowledge-graph-representation-learning-pdf)
    - [MMKG: Multi-Modal Knowledge Graphs [pdf]](#mmkg-multi-modal-knowledge-graphs-pdf)
    - [Multi-modal Knowledge-aware Hierarchical Attention Network for Explainable Medical Question Answering [pdf]](#multi-modal-knowledge-aware-hierarchical-attention-network-for-explainable-medical-question-answering-pdf)
- [Tutorials](#tutorials)
    - [Multimodal Knowledge Graphs: Automatic Extraction & Applications [pdf]](#multimodal-knowledge-graphs-automatic-extraction--applications-pdf)
    - [Towards Building Large-Scale Multimodal Knowledge Bases [pdf]](#towards-building-large-scale-multimodal-knowledge-bases-pdf)
- [Datasets](#datasets)

## Paper

**2015** {#1}

#### Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries [[pdf](https://arxiv.org/pdf/1507.05670.pdf)]
  * Zhu et al. (2015.11)
  * arXiv
  * The complexity of the visual world creates significant challenges for comprehensive visual understanding. In spite of recent successes in visual recognition, today's vision systems would still struggle to deal with visual queries that require a deeper reasoning. We propose a knowledge base (KB) framework to handle an assortment of visual queries, without the need to train new classifiers for new tasks. Building such a large-scale multimodal KB presents a major challenge of scalability. We cast a large-scale MRF into a KB representation, incorporating visual, textual and structured data, as well as their diverse relations. We introduce a scalable knowledge base construction system that is capable of building a KB with half billion variables and millions of parameters in a few hours. Our system achieves competitive results compared to purpose-built models on standard recognition and retrieval tasks, while exhibiting greater flexibility in answering richer visual queries.

**2018** {#2}

#### Towards Building Large Scale Multimodal Domain-Aware Conversation Systems [[pdf](https://arxiv.org/pdf/1704.00200.pdf)]
  * Saha et al. (2018.01)
  * AAAI'18
  * While multimodal conversation agents are gaining importance in several domains such as retail, travel etc., deep learning research in this area has been limited primarily due to the lack of availability of large-scale, open chatlogs. To overcome this bottleneck, in this paper we introduce the task of multimodal, domain-aware conversation, and propose the MMD benchmark dataset. This dataset was gathered by working in close coordination with large number of domain experts in the reail domain. These experts suggested various conversations flows and dialog states which are typically seen in multimodal conversations in the fashion domain. Keeping these flows and states in mind, we created a dataset consisting of over 150K conversation sessions between shoppers and sale agents, with the help of in-house annotators using a semi-automated manually intense iterative process. With this dataset, we propose 5 new sub-tasks for multimodal conversations along with their evaluation methodology. We also propose two multimodal neural models in the encode-attend-decode paradigm and demonstrate their performance on two of the sub-tasks, namely text response generation and best image response selection. These experiments serve to establish baseline performance and open new research directions for each of these sub-tasks. Further, for each of the sub-tasks, we present a 'per-state evaluation' of 9 most significant dialog states, which would enable more focused research into understanding the challenges and complexities involved in each of these states.

#### A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning [[pdf](https://www.aclweb.org/anthology/S18-2027.pdf)]
  * Sergieh et al. (2018.06)
  * SEM'18
  * Current methods for knowledge graph (KG) representation learning focus solely on the structure of the KG and do not exploit any kind of external information, such as visual and linguistic information corresponding to the KG entities. In this paper, we propose a multimodal translation-based approach that defines the energy og a KG triple as the sum of sub-energy functions that leverage both multimodal (visual and linguistic) and structural KG representations. Next, a ranking-based loss is minimized using a simple neural network architecture. Moreover, we introduce a new large-scale dataset for multimodal KG representation learning. We compared the performance of our approach to other baselines on two standard tasks, namely knowledge graph completion and triple classification, using our as well as the WN9-IMG dataset. The results demonstrate that our approach outperforms all baselines on both tasks and datasets.

**2019** {#2}

#### MMKG: Multi-Modal Knowledge Graphs [[pdf](https://arxiv.org/pdf/1903.05485.pdf)]
  * Liu et al. (2019.03)
  * arXiv
  * We present MMKG, a collection of three knowledge graphs that contain both numerical features and (links to) images for all entities as well as entity alignments between pairs of KGs. Therefore, multi-relational link prediction and entity matching communities can benefit from this resource. We believe this data set has the potential to facilitate the development of novel multi-modal learning approaches for knowledge graphs. We validate the utility of MMKG in the sameAs link prediction task with an extensive set of experiments. These experiments show that the task at hand benefits from learning of multiple feature types.

#### Multi-modal Knowledge-aware Hierarchical Attention Network for Explainable Medical Question Answering [[pdf](https://dl.acm.org/doi/10.1145/3343031.3351033)]
  * Zhang et al. (2019.10)
  * ACM-MM'19
  * Online healthcare services can offer public ubiquitous access to the medical knowledge, especially with the emergence of medical question answering websites, where patients can get in touch with doctors without going to hospital. Explainability and accuracy are two main concerns for medical question answering. However, existing methods mainly focus on accuracy and cannot provide good explanation for retrieved medical answers. This paper proposes a novel Multi-Modal Knowledge-aware Hierarchical Attention Network (MKHAN) to effectively exploit multi-modal knowledge graph (MKG) for explainable medical question answering. MKHAN can generate path representation by composing the structural, linguistics, and visual information of entities, and infer the underlying rationale of question-answer interactions by leveraging the sequential dependencies within a path from MKG. Furthermore, a novel hierarchical attention network is proposed to discriminate the salience of paths endowing our model with explainability. We build a large-scale multi-modal medical knowledge graph and two real-world medical question answering datasets, the experimental results demonstrate the superior performance on our approach compared with the state-of-the-art methods. 

## Tutorials

#### Multimodal Knowledge Graphs: Automatic Extraction & Applications [[pdf](http://www.ee.columbia.edu/~sfchang/papers/CVPR2019_MM_Knowledge_Graph_SF_Chang.pdf)]
  * Shih-Fu Chang
  * CVPR'19 & Uni. Columbia

#### Towards Building Large-Scale Multimodal Knowledge Bases [[pdf](https://www.cise.ufl.edu/~dihong/assets/MKBC.pdf)]
  * Dihong Gong
  * Uni. Florida

## Datasets

